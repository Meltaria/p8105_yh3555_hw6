---
title: "p8105_yh3555_hw6"
author: "Yuchen Hua"
date: "2022-11-28"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(mgcv)
library(readr)
library(ggplot2)
library(patchwork)
library(viridis)
set.seed(1)
```

## Problem 1 ##
To download the data. 
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

"modelr::bootstrap" was used to draw the samples and "broom::Glance" was used to produce "r.squared" values. 
```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

From the example, we may find that the r-squared value is high, and the upper bound at 1 may cause the generally skewed shape of the distribution. 
In order to construct a CI for r-squared, the 2.5% and 97.5% quantiles of the estimates would be taken. 
A distribution for log(beta0 * beta1) was also produced. 
```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```
The distribution is skewed and has some outliers. 


## Problem 2 ##
Import data
```{r}
homicide = read.csv("./data./homicide-data.csv")
```

Date cleaning
```{r}
homicide_clean = homicide %>%
  mutate(city_state = str_c(city, ",", state)) %>%
  mutate(city_state = as.factor(city_state)) %>%
  mutate(victim_age = as.numeric(victim_age)) %>%
  group_by(city_state, disposition) %>% 
  filter(!city_state %in% c("Dallas,TX", "Phoenix,AZ", "Kansas City,MO", "Tulsa,AL"),
         !victim_age %in% c(NA),
         !victim_sex %in% c("Unknown")) %>%
  filter(victim_race %in% c("Black", "White")) %>%
  mutate(resolved = ifelse(disposition %in% c("Closed by arrest"), 1, 0)) %>%
  select(-uid, -reported_date, -victim_last, -victim_first, -lat, -lon)
homicide_clean
```
The imported data was cleaned. City and state was joined into a new variable "city_state" via str_c(). City_state was factorized and victim_age was numerized. 
City_state "Dallas,TX", "Phoenix,AZ", "Kansas City,MO", "Tulsa,AL" were excluded. The victim race only include Black and White. 
If the case was closed by arrested, which was solved, it would be labelled as 1. If Open/No arrest or Closed without arrest, it would be labelled as 0. 
Finnaly, some unnecessary variables were excluded. 

For Baltimore,MD,
The data of Baltimore,MD was filered and the glm was built based on the resolved vs victim_age and victim_sex. Broom::tidy was used for better reading. 
```{r}
baltimore_fit = homicide_clean %>%
  filter(city_state %in% c("Baltimore,MD")) %>%
  glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial())
baltimore_fit
baltimore_tidy = broom::tidy(baltimore_fit)
```

From the tidied binary outcome, was can obtained the OR via its estimate. 
The OR and CI of baltimore can be obtained. 
Kinitr::kable was used for a cleared view. 
```{r}
baltimore_tidy %>%
  mutate(OR = exp(estimate),
         ci_lower = exp(estimate - 1.96*std.error),
         ci_upper = exp(estimate + 1.96*std.error)) %>%
  filter(term %in% c("victim_sexMale")) %>%
  select(term, log_OR = estimate, OR, ci_lower, ci_upper) %>%
  knitr::kable(digits = 3)
```


For all cities
Let's repeat the smiliar procedure but utilize map to apply the model building for all the city_state data and unnest the result in the end. Victim_sexMale was chosen via filter as the OR and CI comparing male victims to female victims was required. 
```{r}
nest_city = homicide_clean %>%
  nest(data = -city_state) %>%
  mutate(
    models = map(data, ~glm(resolved ~ victim_age + victim_race + victim_sex, data= ., family = binomial())),
    results = map(models, broom::tidy)) %>%
  select(-data, -models) %>%
  unnest(results) %>%
  filter(term %in% c("victim_sexMale")) %>%
  filter(term != "(Intercept)") %>%
  select(city_state, term, estimate, std.error)
```

Similar procedure was done to obtained OR and CI range. 
```{r}
city_data = nest_city %>%
   mutate(OR = exp(estimate),
         ci_lower = exp(estimate - 1.96*std.error),
         ci_upper = exp(estimate + 1.96*std.error)) %>%
  select(term, log_OR = estimate, OR, ci_lower, ci_upper)
```

Plot was built based on the city data via ggplot. Error bars were created via geom_error bar. 
```{r}
city_data %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>%
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci_lower, ymax= ci_upper)) +
  facet_wrap(~term) + 
  theme(axis.text.x = element_text(angle = 80, hjust = 1))
```



## Problem 3 ##

Import data
```{r}
bw = read.csv("./data/birthweight.csv")
```


Let's eradicate all the missing values in the dataset. All numeric variable's missing data were excluded while the descriptive variables were converted into factors.    
```{r}
bw_tidy = bw %>%
  filter(!bwt %in% c(NA),
         !momage %in% c(NA),
         !delwt %in% c(NA),
         !blength %in% c(NA),
         !fincome %in% c(NA),
         !gaweeks %in% c(NA),
         !menarche %in% c(NA),
         !mheight %in% c(NA),
         !parity %in% c(NA),
         !pnumlbw %in% c(NA),
         !pnumsga %in% c(NA),
         !ppbmi %in% c(NA),
         !ppwt %in% c(NA),
         !smoken %in% c(NA),
         !wtgain %in% c(NA)) %>%
  
  mutate(babysex = as.factor(babysex),
         malform = as.factor(malform),
         mrace = as.factor(mrace),
         frace = as.factor(frace),
         id = 1:nrow(bw))
```

### Building and check hypothesized model ###
The babies' birth weight may be related to their head circumstance and closely related to mother's health condition during pregnancy.
Let's hypothesize that the birthweight has a linear regression relationship with head circumference, mother's weight at delivery, average number of cigarettes smoked per day during pregnancy, weight gained during pregnancy. 
Let's build models. 
```{r}
mod1_lm = lm(bwt ~ blength + delwt + smoken + wtgain, data = bw_tidy)
summary(mod1_lm)
```
According to the summary of the model, we can find out taht all the p-value of the variables are smaller than 0.05, indicating all these variables have a significant association with the birth weight. The adjusted R-squared is 0.5723, explaining 57,23% of the variability in birthweight. 

Let's check the model fit and and the assumption via plotting residuals and fitted values. Add_residuals and add_prediction would be used in the this. 
```{r}
bw_tidy_1 = bw_tidy %>%
  modelr::add_residuals(mod1_lm) %>%
  modelr::add_predictions(mod1_lm)

bw_tidy_1 %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = .5, color = "blue") +
  labs(title = "Residuals vs Predicted",
       x = "Predicted value",
       y = "Residuals")
```
From the plot, we can found that the most of the points are in a type of linearity. There are still some outliers for some extreme predicted values. 

### Cross Validation ### 
Let's build the models of the other two models.
```{r}
mod2_lm = lm(bwt ~ blength + gaweeks, data = bw_tidy)
summary(mod2_lm)
```

```{r}
mod3_lm = lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex, data = bw_tidy)
summary(mod3_lm)
```

Let's compare our models with the other two given models via corss-validation to assess prediction accuracy. 
```{r}
cv_mods = crossv_mc(bw_tidy_1, 1000) %>%
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble))

cv_mods_1 = cv_mods %>%
  mutate(
    mod1_lm = map(train, ~lm(bwt ~ blength + delwt + smoken + wtgain, data = .x)),
    mod2_lm = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    mod3_lm = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex, data = .x))
  ) %>%
  mutate(
    rmse_1 = map2_dbl(mod1_lm, test, ~rmse(model = .x, data = .y)),
    rmse_2 = map2_dbl(mod2_lm, test, ~rmse(model = .x, data = .y)),
    rmse_3 = map2_dbl(mod3_lm, test, ~rmse(model = .x, data = .y))
  )
```

We have got the cross_validation of there three models, and also got the RMSE values too. Thus, we may plot the distribution of RMSE values to check all these 3 models. 
```{r}
cv_mods_1 %>%
  select(starts_with("rmse")) %>%
  pivot_longer(
    everything(),
    names_to = "model", values_to ="rmse", names_prefix = "rmse_") %>%
  ggplot(aes(x = model, y = rmse)) + geom_violin() + 
  labs(title = "RMSE of three models",
       x = "Models",
       y = "RMSE")
```

For the plots shown, the model 3 seems to be most prediction accurate with smallest RMSE. Thus model 3 with head circumference, length, sex and all three interactions can be the most optimal one.  





