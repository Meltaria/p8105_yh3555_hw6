---
title: "p8105_yh3555_hw6"
author: "Yuchen Hua"
date: "2022-11-28"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(modelr)
library(mgcv)
library(readr)
library(ggplot2)
library(patchwork)
library(viridis)
set.seed(1)
```

## Problem 1 
To download the data
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```



## Problem 2
Import data
```{r}
homicide = read.csv("./data./homicide-data.csv")
```

Date cleaning
```{r}
homicide_clean = homicide %>%
  mutate(city_state = str_c(city, ",", state)) %>%
  mutate(city_state = as.factor(city_state)) %>%
  mutate(victim_age = as.numeric(victim_age)) %>%
  group_by(city_state, disposition) %>% 
  filter(!city_state %in% c("Dallas,TX", "Phoenix,AZ", "Kansas City,MO", "Tulsa,AL"),
         !victim_age %in% c(NA),
         !victim_sex %in% c("Unknown")) %>%
  filter(victim_race %in% c("Black", "White")) %>%
  mutate(resolved = ifelse(disposition %in% c("Closed by arrest"), 1, 0)) %>%
  select(-uid, -reported_date, -victim_last, -victim_first, -lat, -lon)
homicide_clean
```
If the case was closed by arrested, which was solved, it would be labelled as 1. If Open/No arrest or Closed without arrest, it would be labelled as 0. 

For Baltimore,MD
```{r}
baltimore_fit = homicide_clean %>%
  filter(city_state %in% c("Baltimore,MD")) %>%
  glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial())
baltimore_fit
baltimore_tidy = broom::tidy(baltimore_fit)
```
The OR and CI of baltimore can be obtained. 
```{r}
baltimore_tidy %>%
  mutate(OR = exp(estimate),
         ci_lower = exp(estimate - 1.96*std.error),
         ci_upper = exp(estimate + 1.96*std.error)) %>%
  filter(term %in% c("victim_sexMale")) %>%
  select(term, log_OR = estimate, OR, ci_lower, ci_upper) %>%
  knitr::kable(digits = 3)
```


For all cities
```{r}
nest_city = homicide_clean %>%
  nest(data = -city_state) %>%
  mutate(
    models = map(data, ~glm(resolved ~ victim_age + victim_race + victim_sex, data= ., family = binomial())),
    results = map(models, broom::tidy)) %>%
  select(-data, -models) %>%
  unnest(results) %>%
  filter(term %in% c("victim_sexMale")) %>%
  filter(term != "(Intercept)") %>%
  select(city_state, term, estimate, std.error)
```

OR an CI
```{r}
city_data = nest_city %>%
   mutate(OR = exp(estimate),
         ci_lower = exp(estimate - 1.96*std.error),
         ci_upper = exp(estimate + 1.96*std.error)) %>%
  select(term, log_OR = estimate, OR, ci_lower, ci_upper)
```

building plot
```{r}
city_data %>% 
  mutate(city_state = fct_reorder(city_state, OR)) %>%
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci_lower, ymax= ci_upper)) +
  facet_wrap(~term) + 
  theme(axis.text.x = element_text(angle = 80, hjust = 1))
```



## Problem 3

Import data
```{r}
bw = read.csv("./data/birthweight.csv")
```


Let's eradicate all the missing values in the dataset. 
```{r}
bw_tidy = bw %>%
  filter(!bwt %in% c(NA),
         !momage %in% c(NA),
         !delwt %in% c(NA),
         !blength %in% c(NA),
         !fincome %in% c(NA),
         !gaweeks %in% c(NA),
         !menarche %in% c(NA),
         !mheight %in% c(NA),
         !parity %in% c(NA),
         !pnumlbw %in% c(NA),
         !pnumsga %in% c(NA),
         !ppbmi %in% c(NA),
         !ppwt %in% c(NA),
         !smoken %in% c(NA),
         !wtgain %in% c(NA)) %>%
  
  mutate(babysex = as.factor(babysex),
         malform = as.factor(malform),
         mrace = as.factor(mrace),
         frace = as.factor(frace),
         id = 1:nrow(bw))
```

Let's hypothesize that the birthweight has a linear regression relationship with head circumference, mother's weight at delivery, average number of cigarettes smoked per day during pregancy, weight gained during pregnancy. 
Let's build models. 
```{r}
mod1_lm = lm(bwt ~ blength + delwt + smoken + wtgain, data = bw_tidy)
summary(mod1_lm)
```
According to the summary of the model, we can find out taht all the p-value of the variables are smaller than 0.05, indicating all these variables have a significant association with the birth weight. The adjusted R-squared is 0.5723, explaining 57,23% of the variability in birthweight. 

Let's check the model fit and and the assumption via plotting residuals and fitted values. Add_residuals and add_prediction would be used in the this. 
```{r}
bw_tidy_1 = bw_tidy %>%
  modelr::add_residuals(mod1_lm) %>%
  modelr::add_predictions(mod1_lm)

bw_tidy_1 %>%
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = .5, color = "blue") +
  labs(title = "Residuals vs Predicted",
       x = "Predicted value",
       y = "Residuals")
```
From the plot, we can found that the most of the points are in a type of linearity. There are still some outliers in some extreme predicted values. 

### Cross Validation ### 
Let's build the models of the other two models.
```{r}
mod2_lm = lm(bwt ~ blength + gaweeks, data = bw_tidy)
summary(mod2_lm)
```

```{r}
mod3_lm = lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex, data = bw_tidy)
summary(mod3_lm)
```

Let's compare our models with the other two given models via corss-validation to assess prediction accuracy. 
```{r}
cv_mods = crossv_mc(bw_tidy_1, 1000) %>%
  mutate(train = map(train, as_tibble),
         test = map(test, as_tibble))

cv_mods_1 = cv_mods %>%
  mutate(
    mod1_lm = map(train, ~lm(bwt ~ blength + delwt + smoken + wtgain, data = .x)),
    mod2_lm = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    mod3_lm = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength + bhead*babysex + blength*babysex, data = .x))
  ) %>%
  mutate(
    rmse_1 = map2_dbl(mod1_lm, test, ~rmse(model = .x, data = .y)),
    rmse_2 = map2_dbl(mod2_lm, test, ~rmse(model = .x, data = .y)),
    rmse_3 = map2_dbl(mod3_lm, test, ~rmse(model = .x, data = .y))
  )
```

We have got the cross_validation of there three models, and also got the RMSE values too. Thus, we may plot the distribution of RMSE values to check all these 3 models. 
```{r}
cv_mods_1 %>%
  select(starts_with("rmse")) %>%
  pivot_longer(
    everything(),
    names_to = "model", values_to ="rmse", names_prefix = "rmse_") %>%
  ggplot(aes(x = model, y = rmse)) + geom_violin() + 
  labs(title = "RMSE of three models",
       x = "Models",
       y = "RMSE")
```

For the plots shown, the model 3 seems to be most prediction accurate with smallest RMSE. Thus model 3 with head circumference, length, sex and all three interactions can be the most optimal one.  





